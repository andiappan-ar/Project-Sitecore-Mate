# --- LLM Provider Configuration ---
# Options: ollama, gemini, openai
LLM_PROVIDER=gemini 

# --- Gemini Configuration (ignored) ---
GEMINI_API_KEY=
GEMINI_DEFAULT_MODEL_GENERAL=
GEMINI_DEFAULT_MODEL_SUMMARY=

# --- OpenAI Configuration (ignored) ---
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_DEFAULT_MODEL_GENERAL=gpt-4o
OPENAI_DEFAULT_MODEL_SUMMARY=gpt-3.5-turbo

# --- Ollama Configuration ---
OLLAMA_BASE_URL=http://localhost:11434 # Ensure Ollama server is running here
OLLAMA_DEFAULT_MODEL_GENERAL=llama2 # Or mistral, llama3 etc.
OLLAMA_DEFAULT_MODEL_SUMMARY=phi3 # A smaller, faster model like phi3 or tinyllama for summarization

# The maximum size of text chunks (in characters) when splitting documents for indexing.
CHUNK_SIZE=1000
# The number of characters that will overlap between consecutive text chunks.
CHUNK_OVERLAP=100
# The number of top relevant results (chunks) to retrieve from the vector database
N_RESULTS=8

# ADVANCED_RAG_ENABLED
ADVANCED_RAG_ENABLED=false